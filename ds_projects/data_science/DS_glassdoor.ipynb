{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT: Data Science Job Posting on Glassdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPLOAD LIBRARIES AND .CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next line loads a CSV file called 'Uncleaned_DS_jobs.csv' into a DataFrame called 'df'. Use ';' as a delimiter, sets the 'id' column as the index and treats '#N/A' as a NaN value in the DataFrame. This code is useful when you need to work with employee data and want to perform analysis using Pandas in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncleaned = pd.read_csv('../../data/Data_science_jobs.csv')\n",
    "df_uncleaned.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncleaned.isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncleaned.isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in a specific column, in this case Job Title\n",
    "column_name = 'Job Title'\n",
    "nan_count = df_uncleaned[column_name].isna().sum()\n",
    "print(f\"Number of NaN values in {column_name}: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the column salary and make it int\n",
    "column_name = 'Salary Estimate'\n",
    "df_uncleaned['lower_salary'] = df_uncleaned[column_name].str.extract(r'\\$(\\d+)K-\\$\\d+K')\n",
    "df_uncleaned['upper_salary'] = df_uncleaned[column_name].str.extract(r'\\$\\d+K-\\$(\\d+)K')\n",
    "# convert columns to int\n",
    "df_uncleaned['lower_salary'] = pd.to_numeric(df_uncleaned['lower_salary'])\n",
    "df_uncleaned['upper_salary'] = pd.to_numeric(df_uncleaned['upper_salary'])\n",
    "# Delete no relevant info columns\n",
    "df_uncleaned.drop(columns='index', inplace=True)\n",
    "df_uncleaned.drop(columns='Salary Estimate', inplace=True)\n",
    "df_uncleaned.drop(columns='Founded', inplace=True)\n",
    "\n",
    "# Show DataFrame result\n",
    "df_uncleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean between upper and lower salary\n",
    "df_uncleaned['Salary_estimate'] = df_uncleaned[['lower_salary', 'upper_salary']].mean(axis=1)\n",
    "\n",
    "#Delete the upper and lower salary nad just leave the mean salary\n",
    "df_uncleaned.drop(columns='lower_salary', inplace=True)\n",
    "df_uncleaned.drop(columns='upper_salary', inplace=True)\n",
    "\n",
    "df_uncleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear the 'Company Name' column and delete everything after \"\\n\".\n",
    "def clean_company_name(company_name):\n",
    "    if \"\\n\" in company_name:\n",
    "        return company_name.split(\"\\n\")[0]\n",
    "    else:\n",
    "        return company_name\n",
    "\n",
    "# Applying the function to the column 'Company Name'.\n",
    "df_uncleaned['Company Name'] = df_uncleaned['Company Name'].apply(clean_company_name)\n",
    "\n",
    "# Get unique values from the column 'Company Name'.\n",
    "unique_company = df_uncleaned['Company Name'].unique()\n",
    "\n",
    "# Showing the first 10 unique values as an example\n",
    "unique_company[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all data equal to \"-1\" and \"Unknown\" by NaN in all the columns\n",
    "df_uncleaned['Headquarters'] = df_uncleaned['Headquarters'].replace(['-1', 'Unknown'], pd.NA)\n",
    "# Get unique values from the 'Size' column\n",
    "unique_size = df_uncleaned['Headquarters'].unique()\n",
    "unique_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all data equal to \"-1\" and \"Unknown\" by NaN in all the columns\n",
    "df_uncleaned['Size'] = df_uncleaned['Size'].replace(['-1', 'Unknown'], pd.NA)\n",
    "# Get unique values from the 'Size' column\n",
    "unique_size = df_uncleaned['Size'].unique()\n",
    "unique_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all data equal to \"-1\" and \"Unknown\" by NaN in all the columns\n",
    "df_uncleaned['Type of ownership'] = df_uncleaned['Type of ownership'].replace(['-1', 'Unknown'], pd.NA)\n",
    "# Get unique values from the 'Size' column\n",
    "unique_size = df_uncleaned['Type of ownership'].unique()\n",
    "unique_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all data equal to \"-1\" and \"Unknown\" by NaN in all the columns\n",
    "df_uncleaned['Industry'] = df_uncleaned['Industry'].replace(['-1', 'Unknown'], pd.NA)\n",
    "# Get unique values from the 'Size' column\n",
    "unique_size = df_uncleaned['Industry'].unique()\n",
    "unique_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all data equal to \"-1\" and \"Unknown\" by NaN in all the columns\n",
    "df_uncleaned['Sector'] = df_uncleaned['Sector'].replace(['-1', 'Unknown'], pd.NA)\n",
    "# Get unique values from the 'Size' column\n",
    "unique_size = df_uncleaned['Sector'].unique()\n",
    "unique_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all data equal to \"-1\" and \"Unknown\" by NaN in all the columns\n",
    "df_uncleaned['Revenue'] = df_uncleaned['Revenue'].replace(['-1', 'Unknown'], pd.NA)\n",
    "# Get unique values from the 'Size' column\n",
    "unique_size = df_uncleaned['Revenue'].unique()\n",
    "unique_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all data equal to \"-1\" and \"Unknown\" by NaN in all the columns\n",
    "df_uncleaned['Competitors'] = df_uncleaned['Competitors'].replace(['-1', 'Unknown'], pd.NA)\n",
    "# Get unique values from the 'Size' column\n",
    "unique_size = df_uncleaned['Competitors'].unique()\n",
    "unique_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all NO relevant words in Description column and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of common English words for exclude\n",
    "common_words_to_exclude = set(['and', 'to', 'the', 'of', 'in', 'a', 'with', 'for', 'or', 'our', 'is', 'we', 'you', 'as', 'on', \n",
    "                               'will', 'are', 'that', 'an', 'be', 's', 'at', 'have', 'this', 'from', 'by', 'other', 'all', 'your', \n",
    "                               'using', 'such', 'more', 'about', 'it', 'their', 'us', 'not', 'but', 'can', 'who', 'them', 'its', \n",
    "                               'also', 'has', 'any', 'into', 'do', 'up', 'out', 'so', 'like', 'if', 'how', 'just', 'which', 'what', \n",
    "                               'some', 'only', 'one', 'no', 'new', 'when', 'here', 'where', 'why', 'would', 'should', 'over', \n",
    "                               'these', 'than', 'then', 'now'])\n",
    "\n",
    "# Joining all job descriptions into a single text string\n",
    "all_descriptions = ' '.join(df_uncleaned['Job Description'])\n",
    "\n",
    "# Clearing the text string: removing non-alphabetic characters and converting to lower case\n",
    "cleaned_descriptions = re.sub(r'\\W+', ' ', all_descriptions).lower()\n",
    "\n",
    "# Dividing the string into individual words and excluding common words.\n",
    "words = [word for word in cleaned_descriptions.split() if word not in common_words_to_exclude]\n",
    "\n",
    "# Counting the frequency of each word.\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Obtaining the 100 most common words\n",
    "most_common_words = word_counts.most_common(100)\n",
    "\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some columns with all the relevant keywords of each postulation job, and transform the use of that word in 0 is not apper in the descrition and give a value of 1 if appers in that postulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords to search for\n",
    "keywords = [\"machine learning\", \"python\", \"sql\", \"excel\", \"hadoop\", \"spark\", \"aws\", \"tableau\", \"power bi\", \"big data\"]\n",
    "\n",
    "# Creating the columns for each keyword\n",
    "for keyword in keywords:\n",
    "    column_name = keyword.replace(\" \", \"_\")\n",
    "    df_uncleaned[column_name] = df_uncleaned['Job Description'].str.contains(keyword, case=False, na=False).astype(int)\n",
    "\n",
    "# Showing the first records to verify the changes\n",
    "df_uncleaned.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_size = df_uncleaned['Size'].unique()\n",
    "unique_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_uncleaned.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
